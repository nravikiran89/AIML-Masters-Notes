Every machine learning algorithm has three components
- Data representation
- Parameter optimization
- Model evaluation, selection

In Practice
- Understand domain, prior knowledge and goals
- Data integration, selection, cleaning, pre-processing, etc
- Learn optimal parameters of the model
- Interpret results
- Consolidate and deploy discoverable knowledge

### Data
- Collection of data objects and attributes
#### Attribute
- Property or characteristic of an object
#### Types of Attributes
##### Discrete Attribute
- Has finite or countably infinite set of values
- Often represented as integer values
- *Binary attributes* are a special case of discrete variables
##### Continuous Attribute
- Has real numbers as attribute values
- Practically, real values can only be measured and represented using finite number of digits
- Continuous attributes are typically represented using finite number of digits
##### Types of Data and Properties
The attributes can take on the following properties

| Attribute | Property                                    | Relation  | Operations                                                     | Transformation                                                                                               | Category                    | Examples                                            |
| --------- | ------------------------------------------- | --------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ | --------------------------- | --------------------------------------------------- |
| Nominal   | Distinctness                                | $= \ \ne$ | mode, entropy, contingency, correlation, $\chi^2$ test         | Any permutation of values                                                                                    | Categorical <br>Qualitative | ID numbers, ZIP codes                               |
| Ordinal   | {Nominal Property} + Order                  | $<\ >$    | median, percentiles, rank, correlation, run tests, sign tests  | An order preserving change of values, i.e., $new_{value} = f(old_{value})$ where $f$ is a monotonic function | Categorical <br>Qualitative | Rankings, Grades, Height{tall, medium, short}       |
| Interval  | {Ordinal Property} + Meaningful differences | $+\ -$    | mean, standard deviation, Pearson's correlation, t and F tests | $new_{values} = a*old_{value}+b$ where a and b are constants                                                 | Numeric <br>Quantitative    | Calendar dates, Temperature in Kelvin or Fahrenheit |
| Ratio     | {Interval Property} + Meaningful ratios     | $*\ /$    | geometric mean, harmonic mean, percent variation               | $new_{value} = a*old_{value}$                                                                                | Numeric <br>Quantitative    | Temperature in Kelvin, length, counts, elapsed time |

#### Object
- Collection of [[#Attribute|attributes]] describe an object
- Also known as record, point, case, sample, entity or instance

#### Data Characteristics
- Dimensionality
	- Number of attributes
- Sparsity
	- Only presence counts
- Resolution
	- Pattern depends on scale
- Size
	- Type of analysis may depend on type of data

#### Data Types
* Relational/Object
* Transactional Data
* Document Data
* Web and Social Network Data
* Spatial Data
* Time Series
* Sequence Data

#### Data Quality
Qualitative measures of data
- Correct
- Interpretable
- Usable on demand
- Complete
- Trustable
- Consistent
Poor quality negatively affects many data processing efforts

=> What kinds of data quality problems?
=> How can we detect problems with data?
=> What can we do about these problems?

Examples:
	Noise or [[3. Descriptive Statistics#Outliers|Outliers]]
	Wrong data
	Fake data
	Missing values
	Duplicate data

##### Noise
For objects, noise is an extraneous object
For attributes, noise refers to modification of original values

##### Outliers
Data Objects with characteristics that are considerably different than most of the other data objects in the data set
- Outliers could be noise that interfere with data analysis
- Outliers could be the goal of our analysis
	- Credit Card fraud
	- Intrusion detection

##### Missing Values
###### Reasons
- Information not collected
- Attributes may not apply to all cases
Handling missing values
- Eliminate data objects or variables
- Estimate missing values
- Ignore missing value during analysis

##### Duplicate Data
- Data set may include objects that are duplicates or almost duplicates of one another
	- Major issue when merging data from heterogenous sources
	- Example: Same person with multiple email addresses
- Data cleaning
	- Process of dealing with duplicate data issues

### Pre-processing
- Involves both data engineering and feature engineering
- Data Engineering: process of converting raw data into prepared data
- Feature Engineering Tune prepared data to create features that are expected by the ML model

=> Stages of Pre-processing
- Data Aggregation
- Data Cleansing
- Instance selection and partitioning
- Feature tuning

#### Aggregation
- Combining two or more attributes into single attribute
- Purpose
	- Data reduction - Reduce number of attributes
	- Change of scale
	- More "stable" data - lower variability
#### Cleansing
- Removing or correcting records of corrupt or invalid values from raw data
	- [[#Noise|Noisy]]: Containing noise, errors, or outliers
	- Inconsistent: Containing discrepancies in codes or names
	- Intentional: Disguised missing data
- Removing records that are missing many [[#Attribute|attributes]]
- Duplicate data
##### Missing Values
- Insert missing values
- Replace with 0s
- Replace with last known values
- Replace with mean
- Interpolate based on splines
##### Handling Outliers
- Using [[3. Descriptive Statistics#Interquartile Range|IQR]]
	- Remove data points outside of $1.5 \times IQR$
- Using [[9. Central Limit Theorem#Z scores for sample means|Z-scores]] methods
	- Remove data points outside of $3\sigma$ deviation

#### Instance selection and Partitioning
Challenge: Non representative data set
Training data should be representative of the new cases we want to generalize
=> Small [[8. Sampling and Distribution#Sample|sample]] size leads to sampling noise. Increase sample size to avoid this problem.
=> If [[8. Sampling and Distribution#Sampling|sampling]] process is flawed, even large sample size can lead to sampling bias.
=> Using a sample which is representative will work almost as well as using the entire data set
=> Representative Sample has approximately same properties as the original set of data

##### Imbalanced Training Set
- Under sample the majority class
- Over sample the rare class

#### Feature Tuning
##### Normalization
- When appropriate upper and lower bounds are known
- When data is approximately uniformly distributed across the range
- When algorithms do not make assumptions about data
- Scales in a range of [0,1] or [-1, 1]
##### Standardization
- When algorithms make assumptions about the data [[7. Continuous Probability Distributions#Normal Distribution|Gaussian Distribution]]
- Not bounded by range
- Less affected by outliers
##### Normalization and Standardization Techniques
###### **Min-Max Normalization**
- Reduce $[A_{min}, A_{max}]$ to $[A_{{min}_{new}}, A_{{max}_{new}}]$
- $v' =\frac{v-A_{min_{new}}}{A_{max_{new}} - A_{min_{new}}}(A_{max_{new}}-A_{min_{new}}) + A_{min_{new}}$
	- EX: If $[A_{min}, A_{max}] = [12000, 98000]$ should be mapped to $[A_{{min}_{new}}, A_{{max}_{new}}] = [0.0, 1.0]$, then 73000 is mapped to $\frac{73000-12000}{98000-12000}(1.0-0.0)+0 = 0.716$
###### **Z-score Normalization/Standardization**
- When $\mu:$ [[1. Measure of Central Tendency#Mean|mean]] $\sigma:$ [[2. Measure of Variability#Standard Deviations|standard\ deviation]] are available.
- $v'=\frac{v - \mu_A}{\sigma_A}$
	- EX: If $\mu = 54000$ and $\sigma=16000$, then $\frac{73000-54000}{16000} = 1.225$
###### **Normalization by decimal scaling**
- $v' = \frac{v}{10^j}$ where j is the smallest integer, such that $max(|v'|) < 1$

### Feature Engineering
- Needed to come up with a good set of features from irrelevant features
- Feature extraction and Dimensionality reduction
- Feature selection
	- Use more useful features to train on among existing features
- Feature construction
	- Combine existing features to produce a more useful one
- Feature Transformation

#### Feature Extraction
=> Curse of dimensionality
- Reducing the number of features by creating lower-dimensions
- When dimensionality increases, data becomes increasingly sparse in the space it occupies
- Solution is to use dimensionality reductions techniques like PCA
#### Feature Selection
- Selecting subset of features for training the model
- Handle redundant features
- Remove irrelevant features
- Dropping features which are missing large number of values
#### Feature Construction
- Create new features by using some techniques
	+ Polynomial expansion (using univariate mathematical functions)
	+ Feature crossing (to capture feature interactions)
	+ Features can be constructed by using business logic from the domain of use case
#### Feature Transformation
- Discretization: Convert continuous attribute to discrete attribute
	- Also known as binning or bucketing
	- Handles outliers
	- Improves value spread
- Involves converting the raw values of a numeric attribute into
	- interval labels
	- conceptual tables
##### Sample Discretization
###### Equal-width partitioning - Continuous Features
- Divides the [[2. Measure of Variability#Range|range]] into N intervals of equal size: uniform grid
- If W is the width of the interval, then $W = \frac{range}{N}$
- Outliers may dominate the presentation
- Skewed data is not handled well
###### Equal-depth partitioning - Continuous Features
- Divides the range into N intervals, each containing approximately same number of samples
- Good data scaling
###### Categorical Encoding - Discrete Features
- Binarization
	- Maps a categorical attribute into one or more binary variables
- Label Encoding
	- Converts categorical features into numerical representation

#MachineLearning #MLWorkFlow