## Supervised Learning - Regression
Wish to learn a function $f:X \to Y$ where predicted output $Y$ is real, given the n real training instance ${<X^1, Y^1>,...,<X^n, Y^n>}$

**GOAL** : Previously unseen records should be assigned a value as accurately as possible.
- Given $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$
- Learn a function $f(x)$ to predict y given x
	- y is real valued

### Learning the Model Parameters
=> Closed Form Solution Approach
=> Gradient Descent Approach

#### Cost function
Given a linear model $$y = \theta_0 + \theta_1 x \tag{1}$$
Cost function is given by $$\begin{align*} J(\theta) &= \frac{1}{2n}\sum_{i=1}^{n}(\hat{y}^{(i)} - y^{(i)})^2 \tag{2} \\
J(\theta) &= \frac{1}{2n}\sum_{i=1}^{n}(h_{\theta}(x^{(i)}) - y^{(i)})^2 \tag{3}\end{align*}$$ 
- Where $h_{\theta}(x)$ is called the Hypothesis
- $\theta_0 \ and \ \theta_1$ are the parameters
- $J(\theta)$ is the cost function

Goal is to minimize the cost function, i.e., $\underset{\theta_0, \theta_1}{minimize} J(\theta_0, \theta_1)$

#### Closed form solution
- Consider the model $h(x) = \sum_{j=0}^{d}(\theta_j x_j)$
- Let, $$\theta = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \cdot \\ \cdot \\ \cdot \\ \theta_d \end{bmatrix} \quad x=\begin{bmatrix}1 \\ x_1 \\ \cdot \\ \cdot \\ \cdot \\ x_d \end{bmatrix}$$
- We can write the model in vectorized form as $h(x) = \theta^T x$

- For a model with multiple features and n instances we can rewrite as $h(x^{(i)}) =\sum_{j=0}{d} \theta_jx_j^{(i)} \tag{4}$
- For Linear regression, cost function is: $$\begin{align*} J(\theta) &= \frac{1}{2n}\sum_{i=1}^{n}(h_{\theta}(x^{(i)}) - y^{(i)})^2 \\
&= \frac{1}{2n}\sum_{i=1}^{n}(\theta^Tx^{(i)} - y^{(i)})^2 \\
&= \frac{1}{2n}(X\theta-y)^T(X\theta-y) \tag{4}\end{align*}$$
- Instead of using Gradient Descent for Solution, we solve for optimal $\theta$ analytically
	- Notice the solution is when $\frac{\partial}{\partial \theta}J(\theta) = 0$
- Derivation:
	- simplifying $(X\theta-y)^T(X\theta-y)$ from 4, $$\begin{align*} 
	\implies &\theta^T X^T X \theta - y^TX \theta - \theta^TX^Ty + y^Ty \\
	= &\theta^T X^T X \theta -2\theta^TX^Ty + y^Ty
	\end{align*}$$
	- Taking derivative of this term and then solving for $\theta$: $$\begin{align*} \frac{\partial}{\partial \theta} (\theta^T X^T X \theta -2\theta^TX^Ty + y^Ty) &= 0 \\ (X^TX)\theta - X^Ty &= 0 \\ (X^TX)\theta &= X^Ty \end{align*}$$
	- Thus, the closed form solution is $$\theta = (X^TX)^{-1}X^Ty \tag{5}$$
#### Gradient Descent
$$J(\theta) = \frac{1}{2n}\sum_{i=1}^{n}(h_{\theta}(x^{(i)}) - y^{(i)}$$
- For insight on J(), let's assume $x \in \mathbb{R}$ so, $\theta = [\theta_0, \theta_1]$
- ![[CostFunction-Intuition.png]]
##### Basic Search Procedure
- Choose initial value for $\theta$
- Until w reach minimum
	- Choose new value for $\theta$ to reduce $J(\theta)$
- Since least squares objective function is convex, we don't need to worry about local minima

##### Algorithm
- Initialize $\theta$
- Repeat until convergence $$\theta_j \leftarrow \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta) \quad simultaneous\ update\ for\ j=0, 1,..., d \tag{6}$$
- For Linear Regression: $$\begin{align*} \frac{\partial}{\partial\theta_j}J(\theta) &= \frac{\partial}{\partial\theta_j}\frac{1}{2n} \sum_{i=1}^{n}(h_{\theta}(x^{(i)}) - y^{(i)})^2 \\ 
 &= \frac{\partial}{\partial\theta_j} \frac{1}{2n}\sum_{i=1}^{n}(\sum_{k=0}^{d}\theta_kx_k^{(i)} - y^{(i)})^2 \\
 &= \frac{1}{n}\sum_{i=1}^{n}(\sum_{k=0}^{d}\theta_kx_k^{(i)} -y^{(i)}) \times \frac{\partial}{\partial\theta_j}(\sum_{k=0}^{d}\theta_kx_k^{(i)}-y^{(i)}) \\
 &= \frac{1}{n}\sum_{i=1}^{n}(\sum_{k=0}^{d}\theta_kx_k^{(i)} - y^{(i)})x_j^{(i)} \end{align*} \tag{7}$$
 ##### Gradient Descent for Linear Regression
 - Initialize $\theta$
 - Repeat until convergence $$\theta_j \leftarrow \theta_j - \alpha\frac{1}{n}\sum_{i=1}^{n}(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \quad simultaneous\ update\ for\ j = 0, ...,d \tag{8}$$
 - To achieve simultaneous update
	 - At the start of each GD iteration, compute $h_{\theta}(x^{(i)})$ 
	 - Use the stored value in the update step loop
- Assume convergence when $||\theta_{new} - \theta_{old}||_2 < \epsilon$ $$L_2\ norm: \quad ||v||_2 = \sqrt{\sum_iv_i^2} = \sqrt{v_1^2+v_2^2+...+v_{|v|}^2} \tag{9}$$
##### Learning Rate
- $\alpha$ too small $\implies$ slow convergence
- $\alpha$ too large $\implies$ Overshoots local minima

#### Closed Form Solution vs Gradient Descent


### Variants of Gradient Descent
#### Batch Gradient Descent
- Calculating the derivative from all training data before calculating an update
#### Minibatch Gradient Descent
- Calculating derivatives of mini groups of training data before calculating an update
#### Stochastic Gradient Descent
- Calculating derivative from each training data instance and calculating the update immediately

### Evaluation Metrics
